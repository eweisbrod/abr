# Merging Data (In Progress)

In this chapter we will learn how to merge in data from various sources. In archival business research, we are interested in companies over time; therefore, we use panel data where the unit of observation is defined by one firm at one point in time in many of our analyses. We will cover different sources of firm-level, panel data and time-series data and how they can be linked together. In this chapter, we will demonstrate best practices using the example project from prior chapters.

## Common Sources of Firm-Level Data

Below is a table of common sources of firm-level and firm identifiers that allow researchers to link data from different data sources:

| Data Source | Primary Identifiers | Other Firm Identifiers | Can Be Linked To | Notes |
|---------------|:-------------:|:-------------:|:-------------:|:--------------|
| **Compustat** | GVKEY | Tic, CIK, CUSIP | CRSP, Audit Analytics, I/B/E/S, TAQ, RavenPack, XBRL, TRACE, Mergent | To link to CRSP data, researchers should use the CRSP-Compustat link file for the GVKEY-PERMNO mapping. |
| **CRSP** | PERMNO | Ticker, CUSIP | Compustat, IBES, TAQ | To link to Compustat data, researchers should use the CRSP-Compustat link file for PERMNO-GVKEY mapping. |
| **I/B/E/S** | TICKER | OFTIC, CUSIP | CRSP, TAQ | The "TICKER" variable is the I/B/E/S firm identifier **not** the trading symbol. The trading symbol is the "OFTIC" variable. |
| **TAQ** | SYMBOL | CUSIP | Compustat, CRSP, I/B/E/S | Researchers should use the TAQ "master" dataset to create SYMBOL-CUSIP mapping. |
| **TRACE** | CUSIP |  | Compustat, CRSP, I/B/E/S |  |
| **Mergent FISD** | ISSUER_ID | CUSIP | Compustat, CRSP, I/B/E/S |  |
| **Audit Analytics** | COMPANY_FKEY |  | Compustat, XBRL | COMPANY_FKEY is Audit Analytic's name for CIK. |
| **XBRL** | CIK |  | Compustat, Audit Analytics |  |
| **RavenPack** | RP_ENTITY_ID | CUSIP, Ticker | Compustat, CRSP, I/B/E/S, TAQ | Researchers should use the "entity mapping file" provided by RavenPack to get CUSIP-RP_ENTITY_ID mapping. |

## Common Sources of Time-Series Data

Sometimes we are interested in variables that are not specific to a particular company, such as asset pricing factors or macroeconomic data. Merging in time-series data from different sources is often easier than firm-level data above because the linking variable between data sources is a point in time variable (e.g., date of the observation). Below are common sources and how to access the data.

## Example: Link in ICMW/Rest/GCO on future performance

So far in our example project, we are exploring the persistence of firm's earnings. Our main hypothesis is that earnings are less persistent for loss firms than profitable firms. Perhaps, we also think this persistence could be moderated by the quality of the financial reporting. To test this, let's grab data that could be useful indicators of poor financial reporting quality from Audit Analytics. We will access this data via WRDS or you can download CSV datasets from Audit Analytics and import into R.

```{r echo=F}
#| label: get AA data
#| 

# Load Libraries [i.e., packages]
library(dbplyr)
library(RPostgres)
library(DBI)
library(glue)
library(arrow)
library(haven)
library(tictoc) #very optional timer, mostly as a teaching example
library(tidyverse) # I like to load tidyverse last to avoid package conflicts


#load helper scripts
#similar to "include" statement in SAS.
source("global-params-packs-util.R")


# Log in to WRDS -------------------------------------------------------------------

#We are using the keyring package to store our passwords
# keyring::key_set("WRDS_user")
# keyring::key_set("WRDS_pw")

#Check if prior connection to WRDS exists.
if(exists("wrds")){ dbDisconnect(wrds)}

wrds <- dbConnect(Postgres(),
                  host = 'wrds-pgdata.wharton.upenn.edu',
                  port = 9737,
                  user = keyring::key_get("WRDS_user"),
                  password = keyring::key_get("WRDS_pw"),
                  sslmode = 'require',
                  dbname = 'wrds')


# Download Audit Analytics Data -------------------------------------------------
#Let's grab restatement data from WRDS. We only need to run this once and save the data locally to use in the future. If we wanted to update for the latest available version, we can uncomment the below to recollect the data and save it down.
#audit.res <- tbl(wrds,in_schema("audit_audit_comp","feed39_financial_restatements")) |> collect()
#write_parquet(audit.res,glue("{data_path}/audit_restate.parquet"))

#Load restatement data and prior data
data <- read_parquet(glue("{data_path}/raw-data-R.parquet"))
restate <- read_parquet(glue("{data_path}/audit_restate.parquet"))

```

Now that we have downloaded the data let's inspect the new restatement data to get a sense of the firm identifiers that we can use to link restatement data to our financial statement data. Without viewing the restatement data, we can see it has 249 variables (columns). Typically, the primary key(s) of the table will be the first few columns, so let's peek at the dataframe.

```{r}
restate |> head()
```

After running the above, we see the first column is called "restatement_notification_key" which is the primary key for this table. In other words, each restatement will have a unique value for this field and should correspond to one row/observations in the data. Lets verify by ensuring there are no duplicate restatement keys. There are a several ways to do this. Examine the code below to confirm there are no duplicate restatement keys.

```{r}
#(1) We can select the column(s) we think make up the primary key for this table and use the command unique() followed by a count() command to see how many rows this returns. We can compare to the original dataframe to see if they are the same. 
restate |> select(restatement_notification_key) |> unique() |> count()
#The above returns 28,456 which matches the number of observations in our original dataset.

#(2) We can group by the primary key(s) and count the number of observations by that grouping to see if there are any with more than one observation. It is good practice to set output of this into a separate dataframe which we are going to call "dups." If there are no duplicate observations this dataset should be empty indicating the restatement key is the primary key. 
dups <- restate |> group_by(restatement_notification_key) |> summarise(obs = n()) |> filter(obs>1)

count(dups)

```

We next will look at how the restatement data is organized to understand how we can link this into our financial statement data. Our financial statement data is organized as a panel data with annual financial statement variables for each firm for each period. However, our restatement data is organized by restatement, and companies (hopefully!) do not restate their financial statements each year. To visualize, lets look at Apple, Inc. in both datasets. Below, we see in our financial statement data Apple appears once per year starting in 1980 with the "datadate" variable representing the end date of each fiscal year's financial statements. In our restatement data, Apple has two restatements.

```{r}
data |> filter(cstat_ticker=="WMT") |> select(cstat_ticker,cik,datadate)

restate |> filter(company_fkey=="0000320193") |> select(restatement_notification_key,file_date,res_begin_date,res_end_date)
```

## Example: Link in FRED Consumer sentiment data

COuld do
